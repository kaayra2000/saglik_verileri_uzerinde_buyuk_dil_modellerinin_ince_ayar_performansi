# GPT-4 hiperparametre aÃ§Ä±klamalarÄ±

* **per_device_train_batch_size:** Her cihazda eÄŸitilecek Ã¶rnek sayÄ±sÄ±dÄ±r. Daha bÃ¼yÃ¼k batch size, daha hÄ±zlÄ± eÄŸitim saÄŸlar ancak daha fazla bellek gerektirir.
* **gradient_accumulation_steps** Modelin parametre gÃ¼ncellemelerinden Ã¶nce kaÃ§ adÄ±mda gradyanlarÄ±n biriktirileceÄŸini belirler. Bu, efektif batch size'Ä± artÄ±rmak iÃ§in kullanÄ±lÄ±r.
* **warmup_steps** Ã–ÄŸrenme oranÄ±nÄ±n tam deÄŸerine ulaÅŸmadan Ã¶nceki adÄ±m sayÄ±sÄ±dÄ±r. Bu, modelin daha stabil Ã¶ÄŸrenmesine yardÄ±mcÄ± olur. Ã–ÄŸrenme oranÄ± Ã§izelgelerinde kullanÄ±lÄ±r ve model eÄŸitiminin baÅŸÄ±nda Ã¶ÄŸrenme oranÄ±nÄ±n kademeli olarak artÄ±rÄ±lmasÄ±nÄ± saÄŸlar.
* **num_train_epochs** EÄŸitim epoch sayÄ±sÄ±,
* **learning_rate** Modelin her adÄ±mda ne kadar bÃ¼yÃ¼k bir adÄ±m atacaÄŸÄ±nÄ± belirler. Ã–ÄŸrenme oranÄ±, eÄŸitim hÄ±zÄ±nÄ± ve stabilitesini etkiler.
* **lr_scheduler_type** Ã–ÄŸrenme oranÄ±nÄ±n zamanla nasÄ±l deÄŸiÅŸeceÄŸini belirler. "linear" gibi scheduler tipleri, Ã¶ÄŸrenme oranÄ±nÄ± belirli bir hÄ±zda dÃ¼ÅŸÃ¼rÃ¼r.
* **logging_steps** EÄŸitim sÄ±rasÄ±nda kaÃ§ adÄ±mda bir eÄŸitim durumunun loglanacaÄŸÄ±nÄ± belirler. EÄŸitim sÃ¼recini izlemeye yardÄ±mcÄ± olur.
* **weight_decay** Model aÄŸÄ±rlÄ±klarÄ±nÄ±n azalmasÄ±nÄ± teÅŸvik ederek aÅŸÄ±rÄ± uyumu (overfitting) Ã¶nlemeye yardÄ±mcÄ± olur.
* **seed** Rastgelelik iÃ§eren iÅŸlemler iÃ§in baÅŸlangÄ±Ã§ deÄŸeri saÄŸlar. Bu, deneylerin tekrarlanabilir olmasÄ±nÄ± saÄŸlar.
* **optim** KullanÄ±lacak optimizasyon algoritmasÄ±nÄ± belirtir. Ã–rneÄŸin, "adamw_8bit" bellek kullanÄ±mÄ±nÄ± optimize eder.
* **fp16** YarÄ± hassas (16 bit) floating point hesaplamalarÄ±nÄ± etkinleÅŸtirir. Bellek kullanÄ±mÄ±nÄ± azaltÄ±r ve hesaplama hÄ±zÄ±nÄ± artÄ±rÄ±r.
* **bf16** Bfloat16 (16 bit) floating point hesaplamalarÄ±nÄ± etkinleÅŸtirir. FP16'ya benzer avantajlar sunar ancak daha stabil olabilir.

# Benim kullanacaÄŸÄ±m deÄŸerler ve sebepleri

* seed=3407 zaten pek mÃ¼him deÄŸil aynÄ± iÅŸin tekrarlanabilmesi iÃ§in
* learning_rate=1e-4
**KAYNAK** Towards Learning Universal Hyperparameter Optimizers with Transformers Figure 1
* warmup_steps=2000 bu tamamen deneysel seÃ§ilecek
* gradient_accumulation_steps cihaza gÃ¶re seÃ§ilecek Ã¶nemsiz
* per_device_train_batch_size cihaza gÃ¶re seÃ§ilecek Ã¶nemsiz
* logging_steps=2000 bilgi amaÃ§lÄ± olduÄŸu iÃ§in Ã¶nemsiz
* save_strategy="steps" eval_strategy="steps" logging_strategy="steps"
* num_train_epochs=1 ya da 2 bu da deneysel
* max_seq_length=modelin boyutu kadar
* lr_scheduler_type="linear" olacak.
* gradient_accumulation_steps=32
* weight_decay=0.01
**KAYNAK** Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task
Except for BART, the decoder for all other models is randomly initialised with the same configuration as the RND
encoder. Since almost every character in the ABC notation is
semantically independent, we took character-level tokenization (but added some common notations), with a vocabulary
size of 164. We trained all models using the same learning
rate Î± = 10âˆ’4
(for BART-large, it is 5Ã—10âˆ’5
), with a 1,000-
step linear warmup and learning rate decay. We trained a total of 20 epochs with a batch size of 8, using the AdamW
optimizer with Î²1 = 0.9, Î²2 = 0.999,  = 10âˆ’8
, and a
weight decay coefficient of 0.01.

* optim="adamw_8bit" kullancam Ã§Ã¼nkÃ¼ eÄŸitim hÄ±zlanÄ±yor, bellek kullanÄ±mÄ± azalÄ±yor ve performans umursanacak seviyede dÃ¼ÅŸmÃ¼yor.
**KAYNAK** 8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION
We develop the first optimizers that use 8-bit statistics while maintaining the performance levels of using 32-bit optimizer states.
In Table 1, we see that 8-bit optimizers match replicated 32-bit performance for all tasks.
The broad range of tasks and competitive results demonstrate that 8-bit optimizers are a robust and effective replacement for 32-bit optimizers, do not require any additional changes in hyperparameters, and save a significant amount of memory while speeding up training slightly.
* bf16=True olacak ve fp16=False Ã§Ã¼nkÃ¼ 
**KAYNAK** Leveraging the bfloat16 Artificial Intelligence Datatype For Higher-Precision Computations
**GENÄ°Å DÄ°NAMÄ°K ARALIK** bfloat16 (BF16) is a new floating-point format [12] that is gaining traction due to its ability to work well in machine learning algorithms, in particular deep learning training. In contrast to the IEEE754-standardized 16bit (FP16) variant, BF16 does not compromise at all on range when being compared to FP32. As a reminder, FP32 numbers have 8 bits of exponent and 24 bits of mantissa (one implicit). BF16 cuts 16 bits from the 24-bit FP32 mantissa to create a 16-bit floating point datatype. In contrast FP16, roughly halves the FP32 mantissa to 10 explicit bits and has to reduce the exponent to 5 bits to fit the 16-bit datatype envelope

**DAHA AZ HASSASÄ°YET KAYBI** Although BF16 offers less precision than FP16, it is better
suited to support deep learning tasks. As shown in [11], FP16â€™s range is not enough to accomplish deep learning training outof-the-box due to its limited range. BF16 does not suffer from this issue and the limited precision actually helps to generalize the learned weights in the neural net training task. In other words, lower precision can be seen as offering a builtin regularization property
**DERIN Ã–ÄRENME Ä°Ã‡Ä°N UYGUNLUK** Additionally, the heart of deep learning is matrix multiplication. That means computing inner products of vectors of various length. Normally the dimensions of these vectors are pretty long: several hundreds to tens of thousands. Therefore, the community has settled on mixed-precision fused-multiply-add (FMA) hardware units.
**KarÄ±ÅŸÄ±k Hassasiyetli Hesaplama** One particular FMA operation that multiplies two BF16 numbers while accumulating in FP32 has been found useful in deep learning, where BF16 is the 16-bit floating point datatype with IEEE FP32 numerical range but 8 significant bits of precision.
**YÃ¼ksek PerformanslÄ± Hesaplamalar**We first demonstrate that computations of vector inner products and by natural extension, matrix-matrix products can be achieved by decomposing FP32 numbers in several BF16 numbers followed by appropriate computations that can accommodate the dynamic range and preserve accuracy compared to standard FP32 computations, while projecting up to 5.2x speed-up.
**Hata Analizi ve Performans Ä°yileÅŸtirmeleri** The first observation one might make is that this last case, a triplet of BF16s, is comparable to FP32 as we have identical range and mantissa bits. Recently such an idea was also employed for NVIDIA Tensorcores with two FP16 numbers for FFT [6]. However more mechanics are needed due to lower ranges of FP16 and only 22 bits total mantissa (if counting the implicit bits.)"
## GPT'nin bf16 vs fp16 vs fp32 yazÄ±sÄ±
### BF16'nÄ±n FP16'ya GÃ¶re AvantajlarÄ±
* GeniÅŸ Dinamik AralÄ±k: BF16, FP32'nin 8 bitlik Ã¼ssÃ¼ ile aynÄ± Ã¼ssÃ¼ kullanÄ±r, bu da FP16'nÄ±n 5 bitlik Ã¼ssÃ¼ne gÃ¶re Ã¶nemli bir avantaj saÄŸlar. Bu sayede BF16, FP32 ile aynÄ± dinamik aralÄ±ÄŸÄ± sunar ve bu da derin Ã¶ÄŸrenme eÄŸitiminde doÄŸrudan kullanÄ±mÄ± mÃ¼mkÃ¼n kÄ±larâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

* Daha Az Hassasiyet KaybÄ±: FP16, 10 bitlik mantisaya sahiptir ve bu da hassasiyet kaybÄ±na yol aÃ§ar. BF16 ise, FP32'nin mantisasÄ±nÄ±n sadece 16 bitini kullanÄ±r, bu da FP16'ya gÃ¶re daha az hassasiyet kaybÄ± demektirâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

* Derin Ã–ÄŸrenme iÃ§in Uygunluk: BF16'nÄ±n daha dÃ¼ÅŸÃ¼k hassasiyeti, derin Ã¶ÄŸrenme gÃ¶revlerinde aÄŸÄ±rlÄ±klarÄ±n genelleÅŸtirilmesine yardÄ±mcÄ± olur, bu da modelin aÅŸÄ±rÄ± Ã¶ÄŸrenmesini (overfitting) engelleyebilirâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

### BF16'nÄ±n FP32 ile Benzer PerformansÄ±
* KarÄ±ÅŸÄ±k Hassasiyetli Hesaplama: BF16, FP32 ile karÄ±ÅŸÄ±k hassasiyetli hesaplamalarda kullanÄ±labilir. Ã–rneÄŸin, iki BF16 sayÄ±sÄ±nÄ± Ã§arparken sonuÃ§ FP32 formatÄ±nda toplanabilir. Bu, 16 bitlik Ã§arpÄ±m sonuÃ§larÄ±nÄ±n tam olarak korunmasÄ±nÄ± ve 24 bit hassasiyetle toplanmasÄ±nÄ± saÄŸlarâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

* YÃ¼ksek PerformanslÄ± Hesaplamalar: BF16, daha kÃ¼Ã§Ã¼k mantisa boyutu nedeniyle daha yÃ¼ksek hesaplama performansÄ± sunar. Ã–rneÄŸin, FP32'ye gÃ¶re 5.2 kat daha hÄ±zlÄ± matris Ã§arpÄ±mlarÄ± (GEMM) gerÃ§ekleÅŸtirebilirâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

* Hata Analizi ve Performans Ä°yileÅŸtirmeleri: BF16 ile yapÄ±lan hesaplamalar, belirli durumlarda FP32'den daha hassas sonuÃ§lar verebilir. ÃœÃ§ BF16 sayÄ±sÄ±nÄ±n birleÅŸtirilmesiyle yapÄ±lan hesaplamalar, FP32 hesaplamalarÄ±na kÄ±yasla benzer veya daha iyi sonuÃ§lar elde edebilirâ€‹(Leveraging_the_bfloat16â€¦)â€‹.

